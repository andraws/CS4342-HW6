{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 40  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluprime(x):\n",
    "    return 1 * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [1, 4],\n",
       "       [1, 5]])"
      ]
     },
     "execution_count": 1667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotten(y):\n",
    "    new_Y = np.zeros((10,y.shape[0]))\n",
    "    for i,val in enumerate(y):\n",
    "        new_Y[val][i] = 1\n",
    "    return new_Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "#     x -= np.max(x,axis=0).reshape(1,-1)\n",
    "    z = np.exp(x) / np.sum(np.exp(x),axis=0)\n",
    "    return z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yHat(x,w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    z1 = np.dot(W1,x.T) + b1[:,None]\n",
    "    print(z1[0])\n",
    "    print()\n",
    "    h = relu(z1)\n",
    "    print(h[0])\n",
    "    z2 = np.dot(W2,h) + b2[:,None]\n",
    "    return softmax(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03234724 -0.05019588  0.18442815  0.1985927   0.16782607]\n",
      "\n",
      "[0.03234724 0.         0.18442815 0.1985927  0.16782607]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0968067 , 0.09043558, 0.10564901, 0.0948646 , 0.10568138,\n",
       "        0.09728182, 0.0986594 , 0.10892358, 0.09916343, 0.10253451],\n",
       "       [0.10131417, 0.08933695, 0.11058904, 0.08544749, 0.10113344,\n",
       "        0.09804517, 0.108772  , 0.11008004, 0.10259179, 0.0926899 ],\n",
       "       [0.09293249, 0.08052613, 0.11565522, 0.0778346 , 0.09684647,\n",
       "        0.11163511, 0.10769651, 0.13863393, 0.09728818, 0.08095137],\n",
       "       [0.09676678, 0.08578867, 0.11119244, 0.08898187, 0.10471422,\n",
       "        0.10176497, 0.09868901, 0.11740705, 0.10064938, 0.09404561],\n",
       "       [0.0851679 , 0.07390499, 0.12312738, 0.08035636, 0.10011125,\n",
       "        0.10060435, 0.10849624, 0.14168244, 0.09946948, 0.08707962]])"
      ]
     },
     "execution_count": 1674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_yHat(trainX[idxs,:],w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    W1 = w[0:NUM_INPUT * NUM_HIDDEN]\n",
    "    W1 = np.reshape(W1, (NUM_HIDDEN, NUM_INPUT))\n",
    "    w = np.delete(w, np.arange(NUM_INPUT * NUM_HIDDEN))\n",
    "    b1 = w[0:NUM_HIDDEN]\n",
    "    w = np.delete(w, np.arange(NUM_HIDDEN))\n",
    "    W2 = w[0:NUM_HIDDEN * NUM_OUTPUT]\n",
    "    W2 = np.reshape(W2, (NUM_OUTPUT, NUM_HIDDEN))\n",
    "    w = np.delete(w, np.arange(NUM_HIDDEN * NUM_OUTPUT))\n",
    "    b2 = w[0:NUM_OUTPUT]\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W = W1.flatten()\n",
    "    W = np.append(W, b1)\n",
    "    W = np.append(W, W2.flatten())\n",
    "    W = np.append(W, b2)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"fashion_mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"fashion_mnist_{}_labels.npy\".format(which))\n",
    "    labels = hotten(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def fCE (X, Y, w):\n",
    "    yhat = get_yHat(X, w) #(5,10)\n",
    "    cost = np.sum((Y*np.log(yhat)))\n",
    "    cost = cost * (-1/Y.shape[0])\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.281532786812229"
      ]
     },
     "execution_count": 1636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fCE((trainX[idxs, :]),(trainY[idxs]), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE (X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    yHat = get_yHat(X,w) #(10,5)\n",
    "    z1 = W1.dot(X.T) + b1[:,None]\n",
    "    h = reluprime(z1)\n",
    "    \n",
    "    gradW2fCE = (yHat - Y).T.dot(h.T) # (10,40)\n",
    "    gradb2fCE = np.mean(yHat - Y,axis=0).reshape(10,1) #(10,1)\n",
    "    \n",
    "    g_Trans = np.multiply(((yHat-Y).dot(W2)),reluprime(z1.T)) #(5,40)\n",
    "    gradW1fCE = g_Trans.T.dot(X) # (40,785)\n",
    "    gradb1fCE = np.mean(g_Trans,axis=0).reshape(40,1) # (40,1)\n",
    "\n",
    "        \n",
    "    return pack(gradW1fCE,gradb1fCE,gradW2fCE,gradb2fCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ...,  0.08717712,\n",
       "       -0.10179899, -0.10632816])"
      ]
     },
     "execution_count": 1647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCE((trainX[idxs, :]),(trainY[idxs]), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "def train (trainX, trainY, testX, testY, w):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1649,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = loadData(\"train\")\n",
    "testX, testY = loadData(\"test\")\n",
    "\n",
    "trainX = trainX/255.00\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "b2 = 0.01 * np.ones(NUM_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31810,)"
      ]
     },
     "execution_count": 1650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all the weights and biases into one vector; this is necessary for check_grad\n",
    "w = pack(W1, b1, W2, b2)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1651,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "fCE_ = fCE((trainX[idxs, :]),(trainY[idxs]), w)\n",
    "gradCE_ = gradCE((trainX[idxs, :]),(trainY[idxs]), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3033446922464607"
      ]
     },
     "execution_count": 1652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fCE(trainX[idxs,:],trainY[idxs],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 1653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCE(trainX[idxs,:],trainY[idxs],w)[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.465743116866799"
      ]
     },
     "execution_count": 1656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "\n",
    "scipy.optimize.check_grad(\n",
    "lambda w_:fCE(trainX[idxs,:],trainY[idxs],w_,),\n",
    "lambda w_:gradCE(trainX[idxs,:],trainY[idxs],w_),\n",
    "w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
