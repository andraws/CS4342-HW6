{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 40  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluprime(x):\n",
    "    return 1 * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotten(x):\n",
    "    return np.eye(NUM_OUTPUT)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row returned sum equals 1, representing probability of its the guess\n",
    "# Returns a (N,10) matrix\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    sum_exp = np.sum(exp,axis=0)\n",
    "    return (exp / sum_exp[None,:]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that performs transformation and gives the softmax of that\n",
    "def get_yHat(X,w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    z1 = W1.dot(X.T) + b1[:,None] # (40,5)\n",
    "    h = relu(z1)    #(40,5)\n",
    "    z2 = W2.dot(h) + b2[:,None] # (10,5)\n",
    "    y_Hat = softmax(z2) #(10.5)\n",
    "    return y_Hat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1076665 , 0.10665026, 0.08690104, 0.08913068, 0.10268867,\n",
       "        0.10442921, 0.09664178, 0.10249014, 0.0920084 , 0.11139332],\n",
       "       [0.11553261, 0.09758634, 0.08084534, 0.08251579, 0.11084937,\n",
       "        0.10490977, 0.10799747, 0.08970749, 0.10157406, 0.10848175],\n",
       "       [0.11504553, 0.10080747, 0.08302698, 0.0952252 , 0.11605032,\n",
       "        0.10165582, 0.10692433, 0.08974994, 0.08898615, 0.10252826],\n",
       "       [0.11992702, 0.09370014, 0.0865971 , 0.09176387, 0.11307983,\n",
       "        0.09605632, 0.09832468, 0.09610266, 0.09781562, 0.10663276],\n",
       "       [0.1154949 , 0.10139947, 0.08964884, 0.08843979, 0.10261602,\n",
       "        0.10958189, 0.09905593, 0.10086694, 0.08865278, 0.10424345]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_yHat(trainX[idxs,:],w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    idx1 = NUM_INPUT*NUM_HIDDEN            # W1\n",
    "    idx2 = idx1 + NUM_HIDDEN               # b1\n",
    "    idx3 = idx2 + NUM_HIDDEN*NUM_OUTPUT    # W2\n",
    "    W1 = w[    : idx1].reshape(NUM_HIDDEN, NUM_INPUT)\n",
    "    b1 = w[idx1: idx2].reshape(NUM_HIDDEN)\n",
    "    W2 = w[idx2: idx3].reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
    "    b2 = w[idx3:     ].reshape(NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    return np.concatenate((W1.flatten(),b1.flatten(),W2.flatten(),b2.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"fashion_mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"fashion_mnist_{}_labels.npy\".format(which))\n",
    "    images = images/255.00\n",
    "    labels = hotten(labels)\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def fCE (X, Y, w):\n",
    "    y_Hat = get_yHat(X,w)\n",
    "    return -np.sum((Y) * np.log(y_Hat)) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.435439167462153"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fCE((trainX[idxs, :]),(trainY[idxs]), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE (X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    y_hat = get_yHat(X,w);\n",
    "\n",
    "    z1 = W1.dot(X.T) + b1[:,None] # (40,5)\n",
    "    h = relu(z1) #(40,5)  \n",
    "\n",
    "    grad_W2 = (y_hat-(Y)).T.dot(h.T) # (10,40)\n",
    "    grad_b2 = np.mean(y_hat-(Y),axis=0) # (10,)\n",
    "\n",
    "    g_T = ((y_hat-(Y)).dot(W2)) * reluprime(z1.T) # (N,40)\n",
    "\n",
    "    grad_W1 = (g_T.T.dot(X))\n",
    "    grad_b1 = np.mean(g_T.T, axis=1) # (40,1)\n",
    "    gradPacked =  pack(grad_W1,grad_b1,grad_W2,grad_b2)\n",
    "    return gradPacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# scipy.optimize.check_grad(\n",
    "# lambda w_:fCE(trainX[idxs,:],trainY[idxs],w_,),\n",
    "# lambda w_:gradCE(trainX[idxs,:],trainY[idxs],w_),\n",
    "# w)\n",
    "# gradCE(trainX[idxs,:],trainY[idxs],w)\n",
    "print_fprim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "def train (trainX, trainY, testX, testY, w):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = loadData(\"train\")\n",
    "testX, testY = loadData(\"test\")\n",
    "\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "b2 = 0.01 * np.ones(NUM_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31810,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all the weights and biases into one vector; this is necessary for check_grad\n",
    "w = pack(W1, b1, W2, b2)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "# fCE_ = fCE((trainX[idxs, :]),(trainY[idxs]), w)\n",
    "# gradCE_ = gradCE((trainX[idxs, :]),(trainY[idxs]), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.651886507445322"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "\n",
    "scipy.optimize.check_grad(\n",
    "lambda w_:fCE(trainX[idxs,:],trainY[idxs],w_,),\n",
    "lambda w_:gradCE(trainX[idxs,:],trainY[idxs],w_),\n",
    "w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "\n",
    "\n",
    "approx_fprime = scipy.optimize.approx_fprime(w, lambda W_: fCE(trainX[idxs,:], trainY[idxs,:], W_), 1e-8)\n",
    "aW1, ab1, aW2, ab2 = unpack(approx_fprime)\n",
    "W1, b1, W2, b2 = unpack(gradCE(trainX[idxs,:], trainY[idxs,:],w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(W1[0,544])\n",
    "print(aW1[0,544])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.14218534 0.02277712 0.         0.03154347 0.11805835\n",
      " 0.01787337 0.         0.         0.01325445 0.02040825 0.28036803\n",
      " 0.09263068 0.06686429 0.         0.07279565 0.16934526 0.00764875\n",
      " 0.01265497 0.10799873 0.         0.11325885 0.09668441 0.12966254\n",
      " 0.00203075 0.01910966 0.15900512 0.01881027 0.10420783 0.1447109\n",
      " 0.         0.01156827 0.04706492 0.         0.         0.12949529\n",
      " 0.         0.         0.16393208 0.        ]\n",
      "[0.         0.02843703 0.00455542 0.         0.00630864 0.02361169\n",
      " 0.00357465 0.         0.         0.00265086 0.00408167 0.05607355\n",
      " 0.01852611 0.01337281 0.         0.01455911 0.03386904 0.00152975\n",
      " 0.00253095 0.02159974 0.         0.02265175 0.01933689 0.0259325\n",
      " 0.00040612 0.00382192 0.03180101 0.00376206 0.02084151 0.02894218\n",
      " 0.         0.00231362 0.009413   0.         0.         0.02589902\n",
      " 0.         0.         0.0327864  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(W2[0])\n",
    "print(aW2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ab2,b2, 1e-3))\n",
    "print(np.allclose(aW1,W1, 1e-3))\n",
    "print(np.allclose(ab1,b1, 1e-3))\n",
    "print(np.allclose(aW2,W2, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fprim():\n",
    "    approx_fprime = scipy.optimize.approx_fprime(w, lambda W_: fCE(trainX[idxs,:], trainY[idxs,:], W_), 1e-8)\n",
    "    aW1, ab1, aW2, ab2 = unpack(approx_fprime)\n",
    "    W1, b1, W2, b2 = unpack(gradCE(trainX[idxs,:], trainY[idxs,:],w))\n",
    "    print(np.allclose(ab2,b2, 1e-3))\n",
    "    print(np.allclose(aW1,W1, 1e-3))\n",
    "    print(np.allclose(ab1,b1, 1e-3))\n",
    "    print(np.allclose(aW2,W2, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
